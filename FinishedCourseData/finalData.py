import json
import datetime
import ast
import requests
import xmltodict
import collections
import bisect
import os
import isodate
import numpy
from math import floor, ceil
from collections import Counter
import math
#import sklearn
#from sklearn import preprocessing
import pylab


APIKEY='API'

def getLengths(codeFile):
    '''Returns a dictionary where keys=video IDs and values=length of video in seconds'''
    code=json.load(open(codeFile))
    v={}
    for i in code.keys():
        information=json.loads(requests.get('https://www.googleapis.com/youtube/v3/videos?part=contentDetails%2Cstatistics&id='+i+'&key='+APIKEY).content)
        if len(information['items'])!=0:
            length=information['items'][0]['contentDetails']['duration']
    	    parsedT=isodate.parse_duration(length)
            secs=parsedT.total_seconds()
            counts=information['items'][0]['statistics']['viewCount']
            v[i]={'length':secs,'counts':counts}
    with open('lengthsAndViews.json', 'w') as outfile:
            json.dump(v, outfile)

#getLengths('videos.json')

def generateVideoDict(fileName):
    '''generates a dictionary where keys are video ids and values are lists of 3-tuples
    (event type, timestamp, current video time). If the events is not play or pause video,
    a 2-tuple is create instead'''
    #(event type,timestamp,current video time,speed) for video events
    filename=json.load(open(fileName))
    video={}
    code=''
    found=False
    current=0
    for i in range(len(filename)):
        if filename[i]['event_type']=='pause_video' or filename[i]['event_type']=='play_video':
            d = json.loads(ast.literal_eval(filename[i]['event']))
            videoDict=dict((k,v) for (k,v) in d.items())
            if code!='' and code!=videoDict['code']:
                video[code].append(('watched another video',datetime.datetime.utcfromtimestamp(filename[i]['timestamp']/1000.0)))            
            code=videoDict['code']
            found=True
            try:
                current=float(videoDict['currentTime'])
                if current<0:
                    raise ValueError
            #when currentTime is not there
            except (KeyError, TypeError,ValueError):
                if filename[i]['event_type']=='pause_video' and filename[i-1]['event_type']=='play_video':
                    diff=datetime.datetime.utcfromtimestamp(filename[i]['timestamp']/1000.0)-datetime.datetime.utcfromtimestamp(filename[i-1]['timestamp']/1000.0)
                    current+=diff.total_seconds()
                elif filename[i]['event_type']=='play_video' and filename[i-1]['event_type']=='pause_video':
                    current=current
                elif filename[i]['event_type']=='play_video' and filename[i-1]['event_type']=='play_video':
                    diff=datetime.datetime.utcfromtimestamp(filename[i+1]['timestamp']/1000.0)-datetime.datetime.utcfromtimestamp(filename[i]['timestamp']/1000.0)
                    current=diff.total_seconds()
                else:
                    continue
            if code not in video:
                video[code]=[(filename[i]['event_type'],datetime.datetime.utcfromtimestamp(filename[i]['timestamp']/1000.0),current,float(videoDict['speed']))]
            else:
                video[code].append((filename[i]['event_type'],datetime.datetime.utcfromtimestamp(filename[i]['timestamp']/1000.0),current,float(videoDict['speed'])))
        else:
            if found:
                video[code].append((filename[i]['event_type'],datetime.datetime.utcfromtimestamp(filename[i]['timestamp']/1000.0)))
                found=False
                code=''
    return video

def parseTimes(listTuples):
    '''Takes a list of 3-tuples and returns a list of intervals. If the current video time is NaN, then
    that event is ignored.'''
    intervals=[]
    type1='play_video'
    type2='pause_video'
    length=len(listTuples)
    i=0
    while i<length-1:
        if listTuples[i][0]==type1:
            start=listTuples[i][2]
            if listTuples[i+1][0]==type2:
                end=listTuples[i+1][2]
            else:
                diff=listTuples[i+1][1]-listTuples[i][1]
                end=start+(listTuples[i][3]*diff.total_seconds())
            if math.isnan(start)==False and math.isnan(end)==False:
                intervals.append([start,end])
            i+=1
        else:
            i+=1
    return intervals
    
#length=json.load(open('Video-Data/length.json'))

def parseAllVid(videoDict):
    '''Takes a dictionary generated by generateVideoDict() and returns a dictionary where the keys are
    video IDs and values are time intervals'''
    all={}
    for key in videoDict:
        try:
            vidLength=length[key]
            intervals=parseTimes(videoDict[key])
            for i in intervals:
                if i[1]>vidLength:
                    i[1]=vidLength
            all[key]=intervals
        except KeyError:
            all[key]=parseTimes(videoDict[key])
    return all

#listOfFileNames=os.listdir('examtakers')

def allUsers(listOfFiles):
    '''Takes in a list of files for each student and writes a json file with
    a dictionary of intervals for each student'''
    for i in listOfFiles:
        d=generateVideoDict('examtakers/'+i)
        combinedD=parseAllVid(d)
        with open('newData/'+i, 'w') as outfile:
            json.dump(combinedD, outfile)

#allUsers(listOfFileNames)                 

def totalTime(dirname):
    '''Takes in a folder with files about each student. Creates a json file with a dictionary
    where keys are student IDs and values are total time they spent watching videos in minutes.
    :param dirname: newData is a folder with student files.'''
    timeDict={}
    listFiles=os.listdir(dirname)
    for i in listFiles:
        oneFile=json.load(open(dirname+'/'+i))
        for key in oneFile:
            #in minutes
            total=(numpy.sum(numpy.diff(oneFile[key])))/60
            if i in timeDict:
                timeDict[i]+=total
            else:
                timeDict[i]=total
    with open('totalTime.json', 'w') as outfile:
        json.dump(timeDict, outfile)

#totalTime('newData')


def countViews(filename):
    '''Takes in a dictionary of intervals and returns a dictionary where the keys are
    current time in video (in seconds) and values are number of rewatches at that time
    :param dirname: newData is a folder with student files.'''
    data = json.load(open(filename))            
    peaksDct = {}
    for key in data.keys():
        values = data[key]
        counterSeg = Counter()
        for seg in values:
            seg = [int(floor(seg[0])), int(ceil(seg[1]))] # rounds the intervals
            end = seg[1]
            for el in range(seg[0], end+1):
                counterSeg[el] += 1
        newC=Counter({ k: v for k, v in counterSeg.iteritems() if v not in (0, 1)}) # ignore seconds with 0 or 1 views
        if len(newC) != 0:
            peaksDct[key] = newC
    return peaksDct   

       
def filterRewatches(dirname):
    '''Creates a json file of dictionaries where the keys are current time 
    in video (in seconds) and values are number of rewatches at that time.
    :param dirname: newData is a folder with student files.'''
    listFiles=os.listdir(dirname)
    for i in listFiles:
        rewatches=countViews(dirname+'/'+i)
        with open('rewatches/'+i, 'w') as outfile:
            json.dump(rewatches, outfile)
            
#filterRewatches('newData')

            
def addRewatches(dirname):
    '''Creates a json file of a dictionary where the keys are user IDs and the values
    are dictionaries with the amount of time rewatched in seconds for each video.
    :param dirname: rewatches is a folder with student files which contain rewatch view counts.'''
    rewatches={}
    listFiles=os.listdir(dirname)
    for i in listFiles:
        oneFile=json.load(open(dirname+'/'+i))
        secs={}
        for key in oneFile:
            total=sum(oneFile[key].values())-len(oneFile[key].values()) # subtract first time from rewatches
            secs[key]=total
        rewatches[i]=secs
    with open('totalRewatchTime.json', 'w') as outfile:
        json.dump(rewatches, outfile)

#addRewatches('rewatches')

def rewatchPeaks(dirname):
    '''Creates a json file of a dictionary where keys are video IDs 
    and values are total rewatch views at each second.
    :param dirname: rewatches is a folder with student files which contain rewatch view counts.'''
    rewatches={}
    listFiles=os.listdir(dirname)
    for i in listFiles:
        oneFile=json.load(open(dirname+'/'+i))
        for videoID in oneFile:
            new=Counter({k:v-1 for k,v in oneFile[videoID].items()}) # subtract 1 to exclude the 1st time
            if videoID not in rewatches:
                rewatches[videoID]=new
            else:
                rewatches[videoID]+=new
    with open('rewatchPeaks.json', 'w') as outfile:
            json.dump(rewatches, outfile)

#rewatchPeaks('rewatches')            
                                    
def getUniqueViews(dirname):
    '''Creates a dictionary where the keys are video IDs and values are unique views.
    :param dirname: newData contains student files'''
    d={}
    listFiles=os.listdir(dirname)
    for i in listFiles:
        oneFile=json.load(open(dirname+'/'+i))
        for video in oneFile:
            if video not in d:
                d[video]=1
            else:
                d[video]+=1
    with open('uniqueViews.json', 'w') as outfile:
        json.dump(d, outfile)

#getUniqueViews('newData')       
                     
def groupUniqueViews(groupFile,uniqueFile):
    '''Creates a json file that adds the number of views for multiple videos that are grouped together
       :param groupFile: videoTitles.json contains the grouped video IDs
        :param uniqueFile: uniqueViews.json contains the number of unique views for each video.'''
    uniqueViews = {}
    ids=json.load(open(groupFile))
    unique=json.load(open(uniqueFile))
    for title in ids:
        try: # only get the group views for videos with transcripts
            total=0
            videoWithTranscript = ids[title]['url'][32:] # get id of video with transcript
            for i in ids[title]['ID']:
                total += unique[i]
            uniqueViews[videoWithTranscript] = total
        except KeyError:
            pass # transcript does not exist
    with open('uniqueGroupViews.json', 'w') as outfile:
        json.dump(uniqueViews, outfile)   

#groupUniqueViews('Video-Data/videoTranscripts/videoTitles.json','Video-Data/FinishedCourseData/uniqueViews.json')


def groupVideos():
    '''Aggregates the views of the videos that are the same (but at different speeds).
    Creates a json file where the 147 keys are video IDs with transcripts and values are views
    at each second'''
    groupCounts = {}
    ids = json.load(open('Video-Data/videoTranscripts/videoTitles.json'))
    length = json.load(open('Video-Data/FinishedCourseData/lengthsAndViews.json'))
    counts = json.load(open('Video-Data/FinishedCourseData/rewatchPeaks.json'))
    for title in ids:
        try:
            videoWithTranscript = ids[title]['url'][32:] # get id of video with transcript
            temp = {}
            vidLen = ids[title]['length'] # length of video with transcript
            for i in ids[title]['ID']: # loop through list of grouped IDs
                vidLen2 = length[i]['length']
                if i != videoWithTranscript:
                    ratio = float(vidLen)/vidLen2
                    for sec in counts[i]:
                        updated = int(round(int(sec)*ratio))
                        if updated not in temp:
                            temp[updated] = counts[i][sec]
                        else:
                            temp[updated] += counts[i][sec]
                else:
                    for sec in counts[i]:
                        if int(sec) not in temp:
                            temp[int(sec)] = counts[i][sec]
                        else:
                            temp[int(sec)] += counts[i][sec]
            groupCounts[videoWithTranscript] = temp
            
        except KeyError: # transcript does not exist
            pass
            
    with open('groupPeaks.json', 'w') as outfile:
            json.dump(groupCounts, outfile)



def getRatio():
    '''Creates a dictionary where keys are seconds and views are ratios (totalView/uniqueUsers)'''
    ratioDict={}
    #viewCounts=json.load(open('Video-Data/FinishedCourseData/groupPeaks.json')) # total number of rewatches in all grouped videos
    viewCounts=json.load(open('Video-Data/FinishedCourseData/pausePlayBins.json'))
    uniqueViewers=json.load(open('Video-Data/FinishedCourseData/uniqueGroupViews.json')) # number of unique users for each video
    for video in viewCounts:
        d={}
        for secs in viewCounts[video]:
            ratio=float(viewCounts[video][secs])/uniqueViewers[video]
            d[secs]=ratio
        ratioDict[video]=d
    with open('pausePlayRatios.json', 'w') as outfile:
        json.dump(ratioDict, outfile)   


def getMaxRatio():
    '''Returns the maximum ratio from ratios.json()'''
    ratios=json.load(open('Video-Data/FinishedCourseData/ratios.json'))
    ratioList=[] # will store max ratio of each video
    for video in ratios:
        maxVal=max(ratios[video].values())
        if maxVal==6917.0:
            print video
        ratioList.append(maxVal)
    return max(ratioList)

def normalize():
    '''Normalizes a set of data by computing ratio/maxRatio. Creates a dictionary 
    where the keys are video IDs and values are the ratios at every second.'''
    ratioDict = {}
    ratios=json.load(open('Video-Data/FinishedCourseData/ratios.json'))
    maxRatio=getMaxRatio()
    for video in ratios:
        one={}
        for sec in ratios[video]:
            one[sec]=float(ratios[video][sec])/maxRatio
        ratioDict[video]=one
    with open('normalize.json', 'w') as outfile:
        json.dump(ratioDict, outfile)

def normalizeTool(filename):
    '''Uses sklearn.preprocessing package to normalize data.
       Creates a dictionary where the keys are video IDs and values are the normalized points at every second.
       :param filename: JSON file with the data (pausePlayBins.json, playBins.json,etc.)''' 
    data=json.load(open(filename))
    normalizedData = data # will update the y values      
    for vid in data: 
        print vid
        y_floats = map(float,data[vid].values())
        normalized = preprocessing.normalize(y_floats)
        print normalized
        keys = data[vid].keys()
        for i in range(len(keys)): # reassign the updated normalized values to each x
            print i
            print keys[i]
            normalizedData[vid][keys[i]] = normalized[i]
    return normalizedData        
 
#normalizeTool('FinishedCourseData/pausePlayBins.json')

                                                                                                                                                                                                                                                                                                      
def getBreaks(filename):
    '''Creates a dictionary where keys are video IDs and values are lists of tuples: 
       (when the break occurred,how long the break lasted).
       Break is defined as pause-play events.
       :param filename: one of the student files in examtakers.'''
    d={}
    events=generateVideoDict(filename)
    for vid in events:
        for i in range(len(events[vid])-1):
            if events[vid][i][0] == 'pause_video':
                if events[vid][i+1][0] == 'play_video':
                    when=events[vid][i][2] # when the break occurs in terms of video time
                    length=(events[vid][i+1][1]-events[vid][i][1]).total_seconds() # calculate difference in datetimes in seconds
                    if vid not in d:
                        d[vid] = [(when,length)]
                    else:
                        d[vid] += [(when,length)]
                        
    return d
            
def aggregateBreaks(dirname): # runtime: 25 minutes
    '''Calls getBreaks() to generate a dictionary where keys are video IDs and 
       values are lists of tuples: (when the break occurred,how long the break lasted).
       :param dirname: examtakers is a directory with student files.'''
    
    ids = json.load(open('Video-Data/videoTranscripts/videoTitles.json'))
    length = json.load(open('Video-Data/FinishedCourseData/lengthsAndViews.json'))
    
    # create dictionary with all break events   
    d={}
    listFiles=os.listdir(dirname)
    for s in listFiles: # loop through each student file
        studentDict=getBreaks(dirname+'/'+s)
        for vid in studentDict:
            if vid not in d:
                d[vid]=studentDict[vid] # append list of tuples to dict
            else:
                d[vid]+=studentDict[vid]
    
    # filter through d to group videos
    for title in ids:
        try:
            videoWithTranscript = ids[title]['url'][32:] # get id of video with transcript
            vidLen = ids[title]['length'] # length of video with transcript
            for i in ids[title]['ID']: # loop through list of grouped IDs
                if i != videoWithTranscript:
                    vidLen2 = length[i]['length']
                    ratio = float(vidLen)/vidLen2
                    for tups in d[i]:
                        updated=(tups[0]*ratio,tups[1])
                        d[videoWithTranscript].append(updated)
                    del d[i]
        except KeyError: # transcript does not exist
            for i in ids[title]['ID']:
                if i in d:
                    del d[i]
                
    with open('pausePlay.json', 'w') as outfile:
            json.dump(d, outfile)

def binning(VIDEOID,lis):
    '''Helper function for breakBins() and countBins(). 
       Returns a list that puts each element of lis into a bin.
       :param lis: list of times or counts'''
    subs=requests.get('http://video.google.com/timedtext?lang=en&v='+VIDEOID).content
    a=xmltodict.parse(subs)
    listOfSubs=a['transcript']['text']
    startTimes=[]
    for i in listOfSubs:
        startTimes.append(float(i['@start']))
    bins=[startTimes[bisect.bisect(startTimes[1:],elt)] for elt in lis]
    return bins

def breakBins(pausePlay): # runtime 1 min
    '''Creates a dictionary where keys are video IDs and values are
       dictionary of break counts at each bin interval.
       :param pausePlay: pausePlay.json is the file generated by aggregateBreaks()''' 
    d={}
    f=json.load(open(pausePlay))
    for vid in f:
        times = [x[0] for x in f[vid]] # create list of when the breaks occurred    
        bins = binning(vid,times)
        binCount = collections.Counter(bins)
        d[vid] = binCount
    with open('pausePlayBins.json', 'w') as outfile:
        json.dump(d, outfile)    
                                                                                                                                      
def countPlays(filename):
    '''Creates a dictionary where keys are video IDs and values are play count lists.
       :param filename: one of the student files in examtakers.'''
    d={}
    events=generateVideoDict(filename)
    for vid in events:
        oneVid=[] # list of when plays occur
        for i in range(len(events[vid])):
            if events[vid][i][0] == 'play_video':
                time = events[vid][i][2] # current time in video
                oneVid.append(time) 
        d[vid]=oneVid
    return d

def countPauses(filename):
    '''Creates a dictionary where keys are video IDs and values are pause count lists.
       :param filename: one of the student files in examtakers.'''
    d={}
    events=generateVideoDict(filename)
    for vid in events:
        oneVid=[] # list of when pauses occur
        for i in range(len(events[vid])):
            if events[vid][i][0] == 'pause_video':
                time = events[vid][i][2] # current time in video
                oneVid.append(time) 
        d[vid]=oneVid
    return d
    
def aggregate(dirname): #runtime 18 min
    '''Calls countPlays() or countPauses() to generate a dictionary where keys are video IDs and 
       values are lists of play/pause events.
       :param dirname: examtakers is a directory with student files.'''
    
    ids = json.load(open('Video-Data/videoTranscripts/videoTitles.json'))
    length = json.load(open('Video-Data/FinishedCourseData/lengthsAndViews.json'))
    
    # create dictionary with all pause/play events  
    d={}
    listFiles=os.listdir(dirname)
    for s in listFiles: # loop through each student file
        studentDict=countPauses(dirname+'/'+s)
        for vid in studentDict:
            if vid not in d:
                d[vid]=studentDict[vid]
            else:
                d[vid]+=studentDict[vid]
    
    # filter through d to group videos
    for title in ids:
        try:
            videoWithTranscript = ids[title]['url'][32:] # get id of video with transcript
            vidLen = ids[title]['length'] # length of video with transcript
            for i in ids[title]['ID']: # loop through list of grouped IDs
                if i != videoWithTranscript:
                    vidLen2 = length[i]['length']
                    ratio = float(vidLen)/vidLen2
                    for elt in d[i]:
                        d[videoWithTranscript].append(ratio*elt)
                    del d[i]
        except KeyError: # transcript does not exist
            for i in ids[title]['ID']:
                if i in d:
                    del d[i]
                
    with open('allPauses.json', 'w') as outfile:
        json.dump(d, outfile) 

def ppBins(allPP): 
    '''Creates a dictionary where keys are video IDs and values are
       dictionary of break counts at each bin interval.
       :param allPP: allPlays.json/allPauses is the file generated by aggregate()''' 
    d={}
    f=json.load(open(allPP))
    for vid in f:
        times = f[vid] # list of when the play/pauses occurred    
        bins = binning(vid,times)
        binCount = collections.Counter(bins)
        d[vid] = binCount
    with open('pauseBins.json', 'w') as outfile:
        json.dump(d, outfile)   

def smooth(x,beta):
    """ kaiser window smoothing """
    window_len=11
    s = numpy.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]
    w = numpy.kaiser(window_len,beta)
    y = numpy.convolve(w/w.sum(),s,mode='valid')
    return y[5:len(y)-5]
 
def getSmoothPoints(filename): 
    '''Creates a dictionary where keys are video IDs and values are
       number of pause/play events are each second. Smoothed values.
       :param filename: data set, dictionary form'''
    videoInfo = json.loads(open(filename).read())
    d={}
    for vid in videoInfo:
        dataX= map(float,videoInfo[vid].keys())
        dataX.sort()
        dataY= []
        for elt in dataX:
            pt = videoInfo[vid][str(elt)]
            dataY.append(pt) 
            
        # x,y are original points (sorted)  
        # x, yy are smoothed points (sorted)       
        x = numpy.array(dataX) # put data in numpy format for smooth()
        y = numpy.array(dataY)
        yy = smooth(y,32) # can be any number depending on how smooth we want it
        # map the 2 lists into dictionaries
        toString = map(str, x.tolist())
        smoothPts = dict(zip(toString,yy.tolist()))
        d[vid]=smoothPts
    with open('FinishedCourseData/pauseBinsSmooth.json', 'w') as outfile:
        json.dump(d, outfile) 

#getSmoothPoints("FinishedCourseData/pauseBins.json")

def addTotalTime(dirname):
    '''Creates a json file of a dictionary where the keys are user IDs and the values
    are dictionaries with the amount of time watched in seconds for each video.
    :param dirname: rewatches is a folder with student files which contain rewatch view counts.'''
    time={}
    listFiles=os.listdir(dirname)
    for i in listFiles:
        oneFile=json.load(open(dirname+'/'+i))
        secs={}
        for key in oneFile:
            total=sum([k[1]-k[0] for k in oneFile[key]])
            secs[key]=total 
        time[i]=secs
    with open('Video-Data/aggregatedTime.json', 'w') as outfile:
        json.dump(time, outfile)

def countPauseLength(filename):
    '''Creates a dictionary where keys are video IDs and values are pause length lists.
       :param filename: one of the student files in examtakers.'''
    d={}
    events=generateVideoDict(filename)
    for vid in events:
        oneVid=[] # list of when pauses occur
        for i in range(len(events[vid])):
            try:
                if (events[vid][i][0] == 'pause_video' and events[vid][i+1][0] == 'play_video') or (events[vid][i][0] == 'pause_video' and events[vid][i+1][0] == 'pause_video'):
                    time = (events[vid][i+1][1]-events[vid][i][1]).seconds # current time in video
                    if time<=100:
                        oneVid.append(time)
            except IndexError:
                pass
        total=sum(oneVid)
        if total!=0:
            d[vid]=total
    return d


def countPauseLengthAll(dirName):
    breakD={}
    listFiles=os.listdir(dirName)
    for i in listFiles:
        d=countPauseLength(dirName+'/'+i)
        breakD[i]=d
    with open('Video-Data/aggregatedPauseLength.json', 'w') as outfile:
        json.dump(breakD, outfile)

def videoEngagement():
    plays=json.loads(open('Video-Data/aggregatedTime.json').read())
    pauses=json.loads(open('Video-Data/aggregatedPauseLength.json').read())
    lens=json.loads(open('Video-Data/FinishedCourseData/lengthsAndViews.json').read())
    counterPlays={user:Counter(plays[user]) for user in plays}
    counterPauses={k:Counter(pauses[k]) for k in pauses}
    total={}
    for user in counterPlays:
        total[user]=counterPlays[user]+counterPauses[user]
    for key in total:
        for vid in total[key]:
            try:
                total[key][vid]=total[key][vid]/lens[vid]['length']
            except KeyError:
                 total[key][vid]='Not Available'
    with open('Video-Data/aggregatedVideoEngagement.json', 'w') as outfile:
        json.dump(total, outfile)

def averageVideoEngagement():
    vE=json.loads(open('Video-Data/aggregatedVideoEngagement.json').read())
    sums={}
    for user in vE:
        for vid in vE[user]:
            if vE[user][vid]!='Not Available':
                if vid not in sums:
                    sums[vid]={'sum': vE[user][vid],'count':1}
                else:
                    sums[vid]['sum']+=vE[user][vid]
                    sums[vid]['count']+=1
    average={k:float(sums[k]['sum'])/float(sums[k]['count']) for k in sums}
    with open('Video-Data/averageVideoEngagement.json', 'w') as outfile:
        json.dump(average, outfile)
    